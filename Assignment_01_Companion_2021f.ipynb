{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 01 Companion 2021f.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hnnDgondBAUr",
        "xftHH_NrDHEp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsh32/machine_learning/blob/main/Assignment_01_Companion_2021f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8eVTQAvA7af"
      },
      "source": [
        "# Assignment 01 Companion Notebook (Machine Learning: Module 1)\n",
        "\n",
        "This notebook contains some examples that show off various properties of linear regression.  This notebook is mostly focused on linear regression from a top-down perspective.  That is, we are concerned with how the algorithm behaves-- not how it is implemented.  As such we will be using a built-in solver for linear regression and treating it essentially as a black box.  (you will be opening that black box later in the assignment). (A \"black box\" is a phrase that means we're going to ignore how things work for now as we investigate the problem. We're just going to be concerned with the inputs and outputs.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9yjSgQyBEeR"
      },
      "source": [
        "## Numpy Practice\n",
        "\n",
        "While you will not be doing any programming in this assignment, in order to understand the examples it helps to have at least a little bit of knowledge of `numpy`.  Additionally, we will be using `numpy` extensively in this course.  Please use the code block below to play around a bit with `numpy` (assuming you aren't already well-versed in `numpy`'s use').\n",
        "\n",
        "In order to help you learn how to use `numpy`, you may consider consulting one of these tutorials.\n",
        "\n",
        "* [Numpy for MATLAB Users](https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html)\n",
        "* [Numpy Cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "298aunatuBP1"
      },
      "source": [
        "# play around with numpy here\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb3T03znbrKE"
      },
      "source": [
        "## Practice making plots\n",
        "Now's the time to make sure you are comfortable making plots in Python. Please play around with the code below, making sure you know what each line does.\n",
        "\n",
        "This example is adapted from [the Python Graph Gallery](https://www.python-graph-gallery.com/), which is a great resource."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eop8VqTHcnNL"
      },
      "source": [
        "# libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a dataset:\n",
        "x_values=np.array([4,8,1,9,3,7])\n",
        "y_values=np.array([14,26,84,12,88,91])\n",
        "\n",
        "# plot\n",
        "plt.plot(x_values, y_values, linestyle='none', marker='o')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAdmRWwqBIAm"
      },
      "source": [
        "## A Toy Linear Regression Problem\n",
        "\n",
        "The notion of a toy problem is very useful for validating that a machine learning algorithm is working as it is intended to. A \"toy problem\" is a very simple version of a more complex problem. Here's an example of a toy linear regression problem.\n",
        "\n",
        "Suppose you are given a learning algorithm designed to estimate some model parameters $\\textbf{w}$ from some training data $(\\mathbf{X}, \\mathbf{y})$.\n",
        "\n",
        "1.  Generate values for the model parameters $\\mathbf{w}$ (e.g., set them to some known values or generate them randomly).  If you were applying your algorithm to real data, you would of course not know these parameters, but instead estimate them from data.  For our toy problem, we'll proceed with values that we generate so we can test our algorithms.\n",
        "\n",
        "2.  Generate some training input data, $\\mathbf{X}$, (random numbers work well for this).  Generate the training output data, $\\mathbf{y}$, by applying the model with parameters $\\mathbf{w}$.  For example, for a linear regression problem if $\\mathbf{w}$ represents the regression coefficients, then we can generate each training label, $y_i$ as $y_i = \\mathbf{x_i}^\\top \\mathbf{w}$.\n",
        "\n",
        "3.  Run your learning algorithms on the synthesized training data $(\\mathbf{X}, \\mathbf{y})$ to arrive at estimated values of the model parameters, $\\hat{\\mathbf{w}}$.\n",
        "\n",
        "4.  Compare $\\mathbf{w}$ and $\\hat{\\mathbf{w}}$ as a way of understanding whether your learning algorithm is working.\n",
        "\n",
        "In the next code block, you'll see an example of a toy regression problem where we set $\\mathbf{w} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, generate some training data, and then recover $\\mathbf{w}$ by applying the linear regression algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSRQjEehBChV"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def linear_regression(X, y):\n",
        "    # The underscores below are because we are ignoring\n",
        "    # those values. You might remember this from SoftDes!\n",
        "    w, _, _, _ = np.linalg.lstsq(X, y, rcond=-1) \n",
        "    # Not sure what \"np.linalg.lstsq\" does? Google it and \n",
        "    # read the documentation!\n",
        "    return w\n",
        "\n",
        "n_points = 50\n",
        "X = np.random.randn(n_points,2)\n",
        "w_true = np.array([1, 2])\n",
        "y = X.dot(w_true)\n",
        "w_estimated = linear_regression(X, y)\n",
        "\n",
        "w_estimated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8WlmVIADq6q"
      },
      "source": [
        "### *Notebook Exercise 1*\n",
        "\n",
        "(a)  What should be true about the relationship between `w_true` and `w_estimated` if the algorithm is working properly?\n",
        "\n",
        "(b)  Are there values of `n_points` that would cause `w_true` to be different from `w_estimated'? (this is a bit tricky, so try some corner cases, values at the extremes, and see if you get a different result.  If you do, try ot understand why.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnnDgondBAUr"
      },
      "source": [
        "#### *Expand for Solution*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJSH6HmkWPry"
      },
      "source": [
        "***Solution***\n",
        "\n",
        "(a) These two vectors should be equal since the inferences of the model should match the true parameters.\n",
        "\n",
        "(b) `n_points = 1` will give you a value for `w_estimated` that doesn't match `w_true`.  This is because them problem is underconstrained (there are more unknowns than equations).  Thus, the linear regression method can find multiple solutions that drive the cost to 0 and has no way of knowing which the correct one was."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIHbn_kGDlqQ"
      },
      "source": [
        "### Investigating Noise\n",
        "\n",
        "One thing you might be interested in knowing is how well the algorithm will work when the data doesn't perfectly conform to the hypothesized model (i.e., $y \\neq \\mathbf{x}^\\top \\mathbf{w}$).\n",
        "\n",
        "In the next code block we'll add some noise to the labels and see if the linear regression algorithm can still reconstruct the true parameters $\\mathbf{w}$.  In particular, we are going to add noise generated from a [Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution) with a specified standard deviation.  By modifying `noise_standard_deviation`, you can explore how the magnitude of the noise influences the quality of the results (we'll be exploring standard deviation and Gaussians in more detail later in this course).  For now you can just think of `noise_standard_deviation` as controlling the magnitude of corruption applied to the training outputs.  \n",
        "\n",
        "Additionally, By modifying `n_points` you can change how many training points are available to the linear regression model for parameter estimation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJCbHyVDBM84"
      },
      "source": [
        "n_points = 50\n",
        "noise_standard_deviation = 2\n",
        "X = np.random.randn(n_points,2)\n",
        "w_true = np.array([1, 2])\n",
        "y = X.dot(w_true) + np.random.randn(n_points,) * noise_standard_deviation\n",
        "w_estimated = linear_regression(X, y)\n",
        "\n",
        "w_estimated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGuUZfqLeBj-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuUrKDpJeDEX"
      },
      "source": [
        "### *Notebook Exercise 2*\n",
        "\n",
        "Please use your plotting skills to plot w_estmated and w_true from above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw5RmLkDrd4I"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhj1v9kNrJdl"
      },
      "source": [
        "#### *Expand for Solution*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgfkASlk7fFU"
      },
      "source": [
        "# plot of w_true vs w_estimated\n",
        "plt.plot(w_true[0],w_true[1], linestyle='none', marker='o')\n",
        "plt.plot(w_estimated[0],w_estimated[1], linestyle='none', marker='x')\n",
        "plt.show()\n",
        "\n",
        "# Here's another plot to help you think about data and regression\n",
        "# What is happening in each of these plots?\n",
        "plt.figure()\n",
        "plt.plot(X[:,0],y,linestyle='none', marker='o') #Original true values\n",
        "plt.plot(X[:,0],X[:,0].dot(w_estimated[0]),linestyle='none', marker='x') #Only partial X\n",
        "plt.plot(X[:,0],X.dot(w_estimated),linestyle='none', marker='*') #Both X components\n",
        "plt.title('First X axis vs 3 forms of y')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X[:,1],y,linestyle='none', marker='o')\n",
        "plt.plot(X[:,1],X[:,1].dot(w_estimated[1]),linestyle='none', marker='x')\n",
        "plt.plot(X[:,1],X.dot(w_estimated),linestyle='none', marker='*')\n",
        "plt.title('Second X axis vs 3 forms of y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUNR0VsNwWzD"
      },
      "source": [
        "### *Notebook Exercise 3*\n",
        "\n",
        "(a)  Keeping `noise_standard_deviation` constant, investigate (qualitatively) the relationship between `n_points` and how close `w_estimated` is to `w_true`. Why do you think this is the case (you don't have the formal language to describe this yet, so try to give a conceptual argument)?\n",
        "\n",
        "(b)  Keeping `n_points` constant, characterize qualitatively the relationship between `noise_standard_deviation` and how close `w_estimated` is to `w_true`? Why do you think this is the case (you don't have the formal language to describe this yet, so try to give a conceptual argument)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xftHH_NrDHEp"
      },
      "source": [
        "#### *Expand for Solution*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0qnMrjw15Zc"
      },
      "source": [
        "***Solution***\n",
        "\n",
        "(a) as `n_points` gets higher `w_estimated` gets closer to `w_true`.  This makes sense intuitively since the random noise added to the training outputs `y` gets averaged out as more training poitns are provided.\n",
        "\n",
        "(b)  as `noise_standard_deviation` gets higher `w_estimated` gets farther from `w_true`.  This makes sense intuitively since the random noise added to each to  training output is greater and thus has the parameter estimates from linear regression are noisier as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ojLVWbDEaHy"
      },
      "source": [
        "### Adding a y-intercept (bias) term\n",
        "\n",
        "The beautiful thing about linear regression is that we can make it work for non-linear functions quite easily.  The easiest way to do this is by augmenting the input data with additional features.  In this way, functions that are non-linear in the original input space become linear in the augmented space.\n",
        "\n",
        "For instance, the function $\\mathbf{w}^\\top \\mathbf{x} + b$ (where $b$ is a scalar or bias term) is non-linear (We know it looks linear but it is actually affine.  If you are not convinced, you can test it against the properties of a [linear map](https://en.wikipedia.org/wiki/Linear_map)).  On the other hand if we construct the vectors $\\mathbf{\\tilde{x}} = \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix}$ and $\\mathbf{\\tilde{w}} = \\begin{bmatrix} \\mathbf{w} \\\\ b \\end{bmatrix}$, then $\\mathbf{w}^\\top \\mathbf{x} + b = \\mathbf{\\tilde{w}}^\\top \\mathbf{\\tilde{x}}$ where $\\mathbf{\\tilde{w}}^\\top \\mathbf{\\tilde{x}}$ is now a linear function!\n",
        "\n",
        "In the code block below, you'll see how this idea can be used to fit a linear regression model to model with a bias term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1rxTjsoD_9n"
      },
      "source": [
        "b = 3\n",
        "y = X.dot(w_true) + b\n",
        "linear_regression(np.hstack((X, np.ones((X.shape[0],1)))), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu03QdvbNM8G"
      },
      "source": [
        "## One more thing\n",
        "\n",
        "You've almost completed your first Machine Learning assignment! Be sure to fill out the survey in Canvas, and then you'll be all done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzZan9g_NYsr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}