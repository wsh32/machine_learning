{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 02a Companion 2021f.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PcJUlOXo-YZE",
        "OfTPP6FAB2sK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsh32/machine_learning/blob/main/modules/02/Assignment_02a_Companion_2021f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRswUMAJ1dE1"
      },
      "source": [
        "#Machine Learning: Module 1 (Fall 2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs0U7R4T1dSK"
      },
      "source": [
        "# Assignment 02a Companion Notebook (first notebook)\n",
        "\n",
        "You've already trained and tested some machine learning models in Matlab! Now you'll try them in Python! We're so excited it's exclamation! points! everywhere!\n",
        "\n",
        "# Iris classification revisited\n",
        "The iris dataset is so famous in machine learning, it's a standard, and comes with Python's most popular machine learning library, sklearn. Let's import that and plot it. This is slightly different than the noisy dataset you were working with earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFhcomV1oYO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X = iris.data  # This convention of X for the data, and \n",
        "y = iris.target #y for the target, is common in ML\n",
        "\n",
        "# Plot the training points\n",
        "# note we're only plotting two of several features!\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
        "            edgecolor='k')\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvJ-gY-H4TNI"
      },
      "source": [
        "We'll need to divide the dataset into training and testing sets. We use the training to teach the machine learning model how to classify, then we use the testing set to see how well our machine learning model learned. \n",
        "\n",
        "Sklearn has a [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WNcvFAt4jIB"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDE-3Xz15OMn"
      },
      "source": [
        "A few notes here. If you read the comments up above, you'll already know that there's a machine learning convention of using X for your features (data) and y for your results (target) that we are following here. This function split the input data into two parts (`X_train`, `X_test`) and the results into two parts (`y_train`, `y_test`). The test sample is about a third of the whole dataset (`test_size=0.33`) and although the data has been divided randomly, we've done it with a fixed random state (`42`) so that no matter how many times you run this cell, the data are divided the same way. This reproducibility is useful as you learn how the code works.\n",
        "\n",
        "Now, we're going to train a simple decision tree model. Please look at [the documentation](https://scikit-learn.org/stable/modules/tree.html) to learn what each variable means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuC9L2fK4jpR"
      },
      "source": [
        "from sklearn import tree\n",
        "#Define the model to have three clusters, and train the model on the training data.\n",
        "clf = tree.DecisionTreeClassifier(min_samples_leaf=3)\n",
        "#Train the model\n",
        "clf = clf.fit(X_train, y_train)\n",
        "#Now take the fitted model and apply it to the test set.\n",
        "y_pred_tree_test=clf.predict(X_test)\n",
        "\n",
        "#Plot\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_tree_test, cmap=plt.cm.Set1)\n",
        "# Uncomment this if you want to directly compare the test data to the output of the decision tree\n",
        "#plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1, marker='x')\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEqCrVUI9rtf"
      },
      "source": [
        "### *Notebook Exercise 1 (15 minutes)*\n",
        "Please consider the following questions.\n",
        "\n",
        "(a) Why does this second plot have fewer points? \n",
        "\n",
        "(b) Can you tell if this classification was accurate, just from the information in this notebook so far?\n",
        "\n",
        "(c) Where did the classification work well? Where did it not work well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcJUlOXo-YZE"
      },
      "source": [
        "####***Expand for Solution***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xESDBgIqAPE_"
      },
      "source": [
        "(a) It has fewer points becasue it is just the testing set, which is 33% of the total number of points.\n",
        "\n",
        "(b) You can compare the colors of the second plot's points to the first plot's points. Where the colors match, the machine learning worked well. Where they are different, the machine learning did not accurately predict the results.\n",
        "\n",
        "(c) The red dots look good. It's a little tricky to tell, but it looks like some of the gray and gold points were hard for the algorithm to distinguish. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eOJBP7YbsAL"
      },
      "source": [
        "### *Notebook Exercise 2 (15 minutes)*\n",
        "\n",
        "Calculate the accuracy of the model on the training and testing data separately.\n",
        "\n",
        "*Hint: You will need to run the model again with X_train as the input to get y_pred_tree_train.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yazdRcIgdUto"
      },
      "source": [
        "####***Expand for Solution***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpFd599hbsgN"
      },
      "source": [
        "y_pred_tree_train=clf.predict(X_train)\n",
        "train_accuracy = np.sum(y_pred_tree_train == y_train)/np.shape(y_pred_tree_train)\n",
        "test_accuracy = np.sum(y_pred_tree_test == y_test)/np.shape(y_pred_tree_test)\n",
        "print('Train accuracy: \\n',train_accuracy)\n",
        "print('Test accuracy: \\n',test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5kKxmJ-ASbH"
      },
      "source": [
        "# You try!\n",
        "\n",
        "### *Notebook Exercise 3 (30 minutes)*\n",
        "\n",
        "Above, we imported a dataset, split it into training and testing sets, and trained a simple machine learning algorithm on it. We'd like you to repeat those three steps here, using a different dataset. At each step, take the time to carefully check what you are doing. This might mean making extra plots, or printing out sections of the data. If you are confused, ask a classmate or an instructor or course assistant to help (either in person, via email, or on discord)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYfB4IuRBSrz"
      },
      "source": [
        "# We'll get you started by importing another commonly used dataset. \n",
        "# This one deals with different characteristics of different types of wine.\n",
        "# From: https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset\n",
        "wine = datasets.load_wine()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfTPP6FAB2sK"
      },
      "source": [
        "# ***Expand for Solution***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQjvWbxmBuoz"
      },
      "source": [
        "# For this, I chose to preserve the X, y, convention for variables.\n",
        "X = wine.data  \n",
        "y = wine.target\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
        "            edgecolor='k')\n",
        "plt.xlabel('Alcohol')\n",
        "plt.ylabel('Malic Acid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv6EGGzNCEpS"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=77)\n",
        "clf = tree.DecisionTreeClassifier(min_samples_leaf=10)\n",
        "#Train the model\n",
        "clf = clf.fit(X_train, y_train)\n",
        "#Now take the fitted model and apply it to the test set.\n",
        "y_pred_tree=clf.predict(X_test)\n",
        "\n",
        "#Plot\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Set1,\n",
        "            edgecolor='k')\n",
        "plt.xlabel('Alcohol')\n",
        "plt.ylabel('Malic Acid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GXrqxNdCtX0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H32mksVC6A9"
      },
      "source": [
        "# Challenge Problem\n",
        "Up next is a totally optional challenge problem, for those who would like it. You may skip it entirely. For those who are interested, consider,\n",
        "1. We only have been plotting two features (for example, sepal length vs. sepal width). But these datasets are multi-dimensional. How could you better visually represent the data? Think of a better way to represent the data, and implement it.\n",
        "2. In this example, we used a decision tree with just one parameter, `min_samples_leaf`. Explore the model by automating a way to check model accuracy vs values of `min_samples_leaf`, or by expanding the number of model parameters used (check the documentation for details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yjMjzVFC7CK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}