{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "2021f Estimating Face Pose from Images.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsh32/machine_learning/blob/main/modules/04/2021f_Estimating_Face_Pose_from_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxJ1nir4k-1P"
      },
      "source": [
        "# Estimating Head Pose from Images\n",
        "\n",
        "Back in 2004, a French grad student spent months taking photos of people's faces at different positions (tilted up/down or left/right). Interestingly, we can train an algorithm to detect face positoins from these photographs. Let's celebrate the difficult and tedious work of that student by using their dataset! \n",
        "\n",
        "First, check out [how the data were collected](https://web.archive.org/web/20190716041053/http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cOdc0iulZgC"
      },
      "source": [
        "# Let's download the data (or upload if using colab). \n",
        "# You may need to click yes to approve running this code if using colab... Can we be trusted?\n",
        "\n",
        "!wget \"https://www.dropbox.com/s/9u9znk0utfr7yjf/HeadPoseImageDatabase.tar.gz?dl=0\" -O head_pose.tar.gz\n",
        "!tar -xvzf head_pose.tar.gz > /dev/null\n",
        "#old: !wget \"https://drive.google.com/uc?authuser=0&id=1304LwlF0o_L0N3njQyB1Gy77FGwUGIUg&export=download\" -O head_pose.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nD8zu1InTlR"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "These data comprise multiple people, each photographed at various head positions (both pitch and yaw).\n",
        "\n",
        "The following code loads the images, crops to the face (face boxes provided in the dataset), and downsamples to 20x20 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JlYRkyIkumB"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "def load_data():\n",
        "    \"\"\" Load the head pose image dataset from here:\n",
        "        http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html\n",
        "        The above website is no longer in commission, but it can be found here:\n",
        "        https://web.archive.org/web/20190716041053/http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html\n",
        "        \n",
        "        returns a tuple containing\n",
        "            person_id: a list of subject directories where the image came from\n",
        "            images: the face boxes cropped to (20, 20) grayscale pixels\n",
        "            head pitch: the pitch of the subject's head\n",
        "            yaw: the yaw of the subject's head \n",
        "            \n",
        "        This version skips pitches more extreme than 30 and skips yaws of 0    \"\"\"\n",
        "    person_ids = []\n",
        "    images = []\n",
        "    pitches = []\n",
        "    yaws = []\n",
        "    for person_path in glob.glob('Person*'):\n",
        "        for image_path in glob.glob(os.path.join(person_path, '*.jpg')):\n",
        "            m = re.search('([-\\+][0-9]*)([-\\+][0-9]*).jpg$', image_path)\n",
        "            pitch = float(m.group(1))\n",
        "            yaw = float(m.group(2))\n",
        "\n",
        "            # don't use images with extreme pitches\n",
        "            if np.abs(pitch) > 30:\n",
        "                continue\n",
        "            # Also remove yaws == 0 to make logistic easier\n",
        "            if yaw == 0:\n",
        "              continue\n",
        "            im = cv2.imread(image_path)\n",
        "            im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "            face_box_path = os.path.join(image_path[:-4] + '.txt')\n",
        "            with open(face_box_path) as f:\n",
        "                lines = f.readlines()\n",
        "                # grab the center pixel coordinate of the face (x_c, y_c) and\n",
        "                # size (w, h) of the face bounding box\n",
        "                x_c, y_c, w, h = (int(l) for l in lines[3:])\n",
        "                # use a square cropping by taking the maximum of the two sizes\n",
        "                big_length = max(w, h)\n",
        "\n",
        "                # grab the face by indexing into the numpy array\n",
        "                face_pixels = im[y_c-big_length//2:y_c+big_length//2,\\\n",
        "                                 x_c-big_length//2:x_c+big_length//2]\n",
        "                try:\n",
        "                    # resize the image to a (20, 20) patch to make it easier\n",
        "                    # for linear regression (less dimensions)\n",
        "                    face_pixels = cv2.resize(face_pixels, (20, 20))\n",
        "                    images.append(face_pixels)\n",
        "                    pitches.append(pitch)\n",
        "                    yaws.append(yaw)\n",
        "                    #print(person_path, yaw, pitch) #uncomment this to troubleshoot\n",
        "                    person_ids.append(person_path)\n",
        "                except Exception as ex:\n",
        "                    continue\n",
        "    return person_ids, np.array(images), np.array(pitches), np.array(yaws)\n",
        "\n",
        "person_ids, images, pitches, yaws = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17a7KeQup4dS"
      },
      "source": [
        "## Visualize the data\n",
        "An important first step in any project is to visualize your data. You want to make sure everything is arranged as expected. The code below plots multiple head positions for the first three people. You can alter this code to look at other examples. \n",
        "\n",
        "We have excluded the examples with the yaw = 0 (looking straight ahead) to make some later exercises more straightforward.\n",
        "\n",
        "**Discuss: What do you notice about the images?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGd01pt6nm0b"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_person(person_id):\n",
        "    fig_scale = 2\n",
        "    fig, ax = plt.subplots(5, 13, figsize=(13*fig_scale, 5*fig_scale))\n",
        "    plt.suptitle(person_id)\n",
        "    subplot_idx = 0\n",
        "    for pitch in np.linspace(-30, 30, 5):\n",
        "        for yaw in np.linspace(-90, 90, 13):\n",
        "            subplot_idx += 1\n",
        "            ax = plt.subplot(5, 13, subplot_idx)\n",
        "            img_idx = np.argwhere(np.logical_and([p == person_id for p in person_ids], np.logical_and(pitches == pitch, yaws == yaw)))\n",
        "            if img_idx.size:\n",
        "                plt.set_cmap('gray')\n",
        "                ax.imshow(images[img_idx[0]].squeeze(), interpolation='none')\n",
        "            ax.set_axis_off()\n",
        "    plt.show()\n",
        "\n",
        "visualize_person('Person01')\n",
        "visualize_person('Person02')\n",
        "visualize_person('Person03')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3137HVIigWLF"
      },
      "source": [
        "## Perform train/test split by group\n",
        "As you observed above, this dataset contains multiple images of the same people looking at different angles. We are now going to do a train/test split. In fact, we're doing a different kind of train/test split. This one will be by group instead of just randomly dividing up the images. This way images from a particular person are only in the training or only in the testing data (but not both). \n",
        "\n",
        "**Discuss: Why might we opt to split by group instead of randomly across all images?**\n",
        "\n",
        "\n",
        "**Discuss: What is point of the line that starts with \" `X = np.hstack(` ...\"?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ne3sFzkWD81"
      },
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "gss = GroupShuffleSplit(test_size=.25, n_splits=1, random_state = 7)\n",
        "indices = np.arange(len(yaws)) \n",
        "\n",
        "for train_idx, test_idx in gss.split(indices, groups=person_ids):\n",
        "#  print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
        "  print(\"TRAIN:\", np.unique(np.array(person_ids)[train_idx]), \"TEST:\", np.unique(np.array(person_ids)[test_idx]))\n",
        "\n",
        "# Reshape the images so each is represented by a vector of pixel values\n",
        "X = images.reshape((images.shape[0], images.shape[1]*images.shape[2]))\n",
        "num_pixels = len(X[0]) # num pixels\n",
        "\n",
        "X = np.hstack((X, np.ones((X.shape[0],1))))\n",
        "\n",
        "# Split the data into train and test based on indices\n",
        "X_train = X[train_idx,:]\n",
        "X_test = X[test_idx,:]\n",
        "person_ids_train = np.array(person_ids)[train_idx]\n",
        "person_ids_test = np.array(person_ids)[test_idx]\n",
        "pitches_train = pitches[train_idx]\n",
        "pitches_test = pitches[test_idx]\n",
        "yaws_train = yaws[train_idx]\n",
        "yaws_test = yaws[test_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v9iZYUs2K-_"
      },
      "source": [
        "## Build a model\n",
        "Read and run the code below.\n",
        "\n",
        "**Discuss: What equation does \"w = ...\" represent?**\n",
        "\n",
        "**Discuss: Do you see any visual pattern in the weights (first plot)?**\n",
        "\n",
        "**Discuss: What should the y-axes label be on the last plot?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDmX4FEZkumE"
      },
      "source": [
        "w = np.linalg.inv(X_train.T.dot(X_train) + 100000000*np.eye(num_pixels+1)).dot(X_train.T.dot(yaws_train))\n",
        "\n",
        "\n",
        "# Plot the weights\n",
        "plt.imshow(w[:-1].reshape((20, 20)))\n",
        "plt.title('Weights as an image')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot the actual and predicted yaws\n",
        "plt.scatter(yaws_train, X_train.dot(w))\n",
        "plt.title('Actual and predicted yaws')\n",
        "plt.xlabel('actual yaw (degrees)')\n",
        "plt.ylabel('predicted yaw (degrees)')\n",
        "plt.show()\n",
        "\n",
        "# Make another plot!\n",
        "plt.scatter(np.arange(len(yaws_train)),X_train.dot(w)-yaws_train,c=pitches_train,cmap=\"jet\")\n",
        "plt.title('A plot to contemplate')\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('What should this label say?')\n",
        "cbar = plt.colorbar()\n",
        "cbar.ax.set_ylabel('Pitches')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEWnDAGlvsyu"
      },
      "source": [
        "In the code above, we combined our data across all pitches (looking up vs straight ahead vs down). \n",
        "\n",
        "Now, let's only consider that data in which the pitch = 0 (looking straight ahead). \n",
        "\n",
        "**Discuss: Why might we opt to do this?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNRy9-BE1Qr3"
      },
      "source": [
        "## Analyze when looking straight ahead (pitch = 0)\n",
        "\n",
        "Investigate the relationship between the variable lam (for $\\lambda$) in the code below. As before, try changing it over several orders of magnitude.\n",
        "\n",
        "**Discuss: What happens when your value for lam is much lower or higher than starting value? Consider this in relation to the 3 plots. What happens to the fit of the model (e.g., when is it overfit or underfit)?**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOhuZpxykumJ"
      },
      "source": [
        "pitch_value = 0\n",
        "lam = 10000000  #10000000 was starting point\n",
        "X_restricted = X_train[pitches_train == pitch_value, :]\n",
        "w = np.linalg.inv(X_restricted.T.dot(X_restricted) + lam*np.eye(X.shape[1])).dot(X_restricted.T.dot(yaws_train[pitches_train == pitch_value]))\n",
        "\n",
        "# Plot weights\n",
        "plt.imshow(w[:-1].reshape((20, 20)))\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Plot Predicted and Actual yaw\n",
        "plt.scatter(yaws_train[pitches_train == pitch_value], X_restricted.dot(w))\n",
        "plt.xlabel('actual yaw (degrees)')\n",
        "plt.ylabel('predicted yaw (degrees)')\n",
        "plt.show()\n",
        "\n",
        "# Plot residuals\n",
        "plt.scatter(np.arange(len(yaws_train[pitches_train == pitch_value])),X_restricted.dot(w)-yaws_train[pitches_train == pitch_value])\n",
        "plt.xlabel('Data point')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k5KAp9terM3"
      },
      "source": [
        "## Plotting pixels\n",
        "Sometimes when working with image data, it can be a little tricky to visualize. Above, we looked at the plots of our weights. Here, we'll look at two plots, where each plot shows the value of two different pixels. Each dot represents a different face image. In the top plot, we randomly picked two pixel (feel free to change the numbers). In the second plot, we selected the pixels based on their weights.\n",
        "\n",
        "**Discuss: Why does the second plot show more distinct differences?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGM458Pbeuhs"
      },
      "source": [
        "idx_w_max= np.where(w==np.max(w))\n",
        "#print(idx_w_max, ' idx for ', w[idx_w_max])\n",
        "idx_w_min = np.where(w==np.min(w))\n",
        "#print(idx_w_min, ' idx for ', w[idx_w_min])\n",
        "\n",
        "plt.scatter(X_restricted[:,10],X_restricted[:,231],c=yaws_train[pitches_train == pitch_value],cmap=\"jet\")\n",
        "plt.title('Pixels randomly selected')\n",
        "plt.xlabel('Pixel value for random pixel (10) ')\n",
        "plt.ylabel('Pixel value for random pixel (231)')\n",
        "cbar = plt.colorbar()\n",
        "cbar.ax.set_ylabel('Yaws')\n",
        "plt.show()\n",
        "\n",
        "#ax2 = fig.add_subplot(133,aspect='equal')\n",
        "plt.scatter(X_restricted[:,idx_w_min],X_restricted[:,idx_w_max],c=yaws_train[pitches_train == pitch_value],cmap=\"jet\")\n",
        "plt.title('Pixels selected by min and max weight')\n",
        "plt.xlabel('Pixel value for pixel w/ min weight')\n",
        "plt.ylabel('Pixel value for pixel w/ max weight')\n",
        "cbar = plt.colorbar()\n",
        "cbar.ax.set_ylabel('Yaws')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BQBYT9thsul"
      },
      "source": [
        "## Consider the model and data\n",
        "\n",
        "Now that you've had a chance to work with these data, refer back to the [original dataset](https://web.archive.org/web/20190716041053/http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html) and data collection protocol. Check out the photos of their setup. \n",
        "\n",
        "**Discuss: What are some of the limitations of this data set and/or the linear regression model?**\n",
        "\n",
        "\n",
        "If you have not seen it previously, you may also want to check out the [Gender Shades Project](http://gendershades.org/overview.html). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6EB8Y6ZIDK6"
      },
      "source": [
        "# Motivating logistic regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN2GU_DDFOkq"
      },
      "source": [
        "You may have noticed in the plot above that the yaws appeared to fall into two groups: looking to the left or looking to the right (i.e., clusters of similar colors).\n",
        "\n",
        "Now, we will explore the same data from a binary classification framing and apply linear regression. For binary classification, we'll need to consolidate our output from a range of yaw values to a binary set of outputs (bin_yaws_train).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR27A7nSlwPB"
      },
      "source": [
        "# Make new outputs with binary 1 for positive yaws, 0 for negative\n",
        "bin_yaws_train = np.zeros_like(yaws_train)\n",
        "bin_yaws_train[yaws_train>0] = 1\n",
        "bin_yaws_test = np.zeros_like(yaws_test)\n",
        "bin_yaws_test[yaws_test>0] = 1\n",
        "\n",
        "#histograms to confirm that the outputs are now binary\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(yaws_train)\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(bin_yaws_train)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoVQlUTt-KMV"
      },
      "source": [
        "##Run logistic regression\n",
        "Now let's run logistic regression on our binary outputs. \n",
        "\n",
        "Study the code and plots below to understand what is happening. \n",
        "\n",
        "Then manipulate C, which is the inverse of the regularization strength. \n",
        "\n",
        "**Discuss: What happens to the plots and accuracy as you manipulate C?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GObZWPeLIGQ4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def fit_model(X_input,y_input):\n",
        "    model = LogisticRegression(max_iter=10000,C=0.0000001) #starts at C=0.0000001\n",
        "    model.fit(X_input, y_input)\n",
        "    return model\n",
        "\n",
        "model = fit_model(X_train,bin_yaws_train);\n",
        "pred_bin_yaws_train = model.predict(X_train) #This gives a prediction as a binary\n",
        "prob_bin_yaws_train = model.predict_proba(X_train) #This gives the probability of each output (0,1)\n",
        "\n",
        "# Plot weights\n",
        "mc = np.squeeze(model.coef_)\n",
        "plt.imshow(mc[:-1].reshape((20, 20)))\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "#Plot histogram of outputs\n",
        "plt.figure()\n",
        "plt.hist(prob_bin_yaws_train[bin_yaws_train==0,1],alpha=0.7)\n",
        "plt.hist(prob_bin_yaws_train[bin_yaws_train==1,1],alpha=0.7)\n",
        "plt.legend(['y=0','y=1'])\n",
        "plt.xlabel('predicted probability')\n",
        "plt.ylabel('counts')\n",
        "plt.title('Histogram of probabiltity outputs by true label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('Accuracy on training set', (pred_bin_yaws_train == bin_yaws_train).mean())\n",
        "print('Accuracy on testing set', (model.predict(X_test) == bin_yaws_test).mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3LAqxI5QvYm"
      },
      "source": [
        "Finally, we'll plot the values from a couple of pixels again with the colors based on our binary categories. The pixels to plot are based on the most extreme weights from the linear and logistic regression models.\n",
        "\n",
        "**Discuss: What do you observe about these plots? What happens when C=0.0000001 vs something else? What does this suggest?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr0t6Mv7iGIh"
      },
      "source": [
        "idx_mc_max= np.where(mc==np.max(mc))\n",
        "print(idx_mc_max, ' idx for ', mc[idx_mc_max])\n",
        "idx_mc_min = np.where(mc==np.min(mc))\n",
        "print(idx_mc_min, ' idx for ', mc[idx_mc_min])\n",
        "\n",
        "plt.scatter(X_train[:,idx_mc_min],X_train[:,idx_mc_max],c =bin_yaws_train ,cmap=\"jet\")\n",
        "plt.xlabel('Pixel value for pixel w/ min weight')\n",
        "plt.ylabel('Pixel value for pixel w/ max weight')\n",
        "plt.title('Pixels with most extreme weights from logistic')\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(X_train[:,idx_w_min],X_train[:,idx_w_max],c =bin_yaws_train ,cmap=\"jet\")\n",
        "plt.xlabel('Pixel value for pixel w/ min weight')\n",
        "plt.ylabel('Pixel value for pixel w/ max weight')\n",
        "plt.title('Pixels with most extreme weights from linear')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKPYBMYfH1PK"
      },
      "source": [
        "That's all for this notebook. Optionally, you can go back and do the same analysis but splitting into a binary classification of looking ahead vs looking to the side (threshold at around 45 degrees). Or you can just move on to the assignment.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7cadbqBdd9Z"
      },
      "source": [
        "##Remember to do your reflection on Canvas!"
      ]
    }
  ]
}